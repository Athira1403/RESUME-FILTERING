{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import os,os.path\n",
    "import gensim,re \n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "import pandas as pd\n",
    "import math\n",
    "re_c = re.compile(r'\\w+')\n",
    "import spacy\n",
    "import numpy as np\n",
    "from spacy.vectors import Vectors\n",
    "vectors = Vectors(shape=(10000, 300))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab.vectors = vectors\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "    sent_file = open(('sentences.txt'),'w',encoding=\"utf8\")\n",
    "    with open(\"para.txt\",encoding=\"utf8\") as fobj:\n",
    "        for line in fobj:\n",
    "            if line != '\\n' and line.strip().startswith('<p>'):\n",
    "                try:\n",
    "                    soup = BeautifulSoup(line.strip(),\"lxml\")\n",
    "                    doc = nlp(soup.p.text)\n",
    "                except:\n",
    "                    logging.warning(line,\"can't be parsed.\")\n",
    "                    continue\n",
    "                for each in doc.sents:\n",
    "                    text = each.text+'\\n'\n",
    "                    sent_file.write(each.text+'\\n')\n",
    "                    \n",
    "    sent_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __iter__(self):\n",
    "        with open(('sample_bitcoin.stackexchange_sentences.txt'),'r', encoding='utf-8') as fin:\n",
    "            for line in fin:\n",
    "                review_text = re.sub(\"[^a-zA-Z]\",\" \", line)\n",
    "                print(review_text)\n",
    "                yield review_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MySentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MySentences at 0x265c0b02588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(lambda : 0)\n",
    "for each in data:\n",
    "    d[len(each)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = dict(sorted(d.items(), key=operator.itemgetter(1), reverse=True)[:10])\n",
    "plt.bar(range(len(d)), d.values(), align='center')\n",
    "plt.xticks(range(len(d)), d.keys())\n",
    "plt.ylabel(\"Number of sentences\")\n",
    "plt.xlabel(\"Number of words in that sentence\")\n",
    "plt.savefig('context.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(data,workers=4, size=300, min_count = 1, window = 15, sample = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"model\"\n",
    "# model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_print = True\n",
    "flag_clear = True\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_to = {\n",
    "    'edu' : ['education', 'study', 'academics', 'institute', 'school', 'college'],\n",
    "    'exp' : ['job', 'internship', 'training', 'research', 'career', 'profession', 'role'\n",
    "             'project', 'responsibility', 'description', 'work experience', 'workshop', 'conference'],\n",
    "    'skill' : ['skill', 'languages', 'technology', 'framework', 'tools', 'database'],\n",
    "    'extra' : ['introduction', 'intro', 'achievement', 'hobby', 'links', 'additional', \n",
    "               'personal', 'award', 'objective', 'miscellaneous', 'interest']\n",
    "}\n",
    "\n",
    "list_of_sections = similar_to.keys()\n",
    "for section in list_of_sections:\n",
    "    new_list = []\n",
    "    \n",
    "    for word in similar_to[section]:\n",
    "        docx = nlp(word)\n",
    "        new_list.append(docx[0].lemma_)\n",
    "        \n",
    "    if flag_print:\n",
    "        print(section, new_list)\n",
    "        \n",
    "    similar_to[section] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify(word):\n",
    "    try:\n",
    "        symbols = '''~'`!@#$%^&*)(_+-=}{][|\\:;\",./<>?'''\n",
    "        mod_word = ''\n",
    "        \n",
    "        for char in word:\n",
    "            if (char not in symbols):\n",
    "                mod_word += char.lower()\n",
    "\n",
    "        docx = nlp(mod_word)\n",
    "\n",
    "        if (len(mod_word) == 0 or docx[0].is_stop):\n",
    "            return None\n",
    "        else:\n",
    "            return docx[0].lemma_\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "if flag_print:\n",
    "    test_words = ['Hello!!', '.,<>', 'India', 'of', '..freedoM..', 'e-mail']\n",
    "    \n",
    "    for word in test_words:\n",
    "        print(word, '--returned-->', modify(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty(line):\n",
    "    for c in line:\n",
    "        if (c.isalpha()):\n",
    "            return False\n",
    "    return True\n",
    "      \n",
    "if flag_print:\n",
    "    test_words = ['.', '<.>', 'Speak', 'out', '\"Eric\"', 'freemail...']\n",
    "    \n",
    "    for word in test_words:\n",
    "        print(word, '--returned-->',) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_data_series = {}\n",
    "flag_print = False\n",
    "for file_name in os.listdir(os.getcwd()+'/CVs'):\n",
    "    if flag_print:\n",
    "        print('\\n')\n",
    "        print('*'*25) \n",
    "        print(file_name) \n",
    "        print('*'*25) \n",
    "    main_file_handler = open('CVs/'+file_name, 'r', encoding='latin-1')  \n",
    "    previous_section  = 'extra'\n",
    "    curr_data_series = pd.Series([\"\"]*len(list_of_sections), index=list_of_sections)             \n",
    "    for line in main_file_handler:\n",
    "        if (len(line.strip()) == 0 or is_empty(line)):\n",
    "            continue\n",
    "        list_of_words_in_line = re_c.findall(line)\n",
    "        list_of_imp_words_in_line  = []\n",
    "        for i in range(len(list_of_words_in_line)):\n",
    "            modified_word = modify(list_of_words_in_line[i])\n",
    "            if (modified_word):\n",
    "                list_of_imp_words_in_line.append(modified_word)\n",
    "        curr_line = ' '.join(list_of_imp_words_in_line)\n",
    "        doc = nlp(curr_line)\n",
    "        section_value = {}\n",
    "        for section in list_of_sections:\n",
    "            section_value[section] = 0.0\n",
    "        section_value[None] = 0.0\n",
    "        for token in doc:\n",
    "            for section in list_of_sections:\n",
    "                for word in similar_to[section]:\n",
    "                    word_token = doc.vocab[word]\n",
    "                    section_value[section] = max(section_value[section], float(word_token.similarity(token)))\n",
    "        most_likely_section = None\n",
    "        for section in list_of_sections:\n",
    "            if (section_value[most_likely_section] < section_value[section] and section_value[section] > threshold):\n",
    "                most_likely_section = section\n",
    "        if (previous_section != most_likely_section and most_likely_section is not None):\n",
    "            previous_section = most_likely_section\n",
    "        try:\n",
    "            docx = nlp(line)\n",
    "        except:\n",
    "            continue  \n",
    "        mod_line = ''\n",
    "        for token in docx:\n",
    "            if (not token.is_stop):\n",
    "                mod_line += token.lemma_ + ' '\n",
    "        curr_data_series[previous_section] += mod_line\n",
    "    dict_of_data_series[file_name] = curr_data_series\n",
    "    if flag_print:\n",
    "        print(curr_data_series)\n",
    "    main_file_handler.close()\n",
    "data_frame = pd.DataFrame(dict_of_data_series)\n",
    "# data_frame.to_csv('prc_data.csv', sep='\\t')\n",
    "# data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(word, n): \n",
    "    word = word.lower()\n",
    "    words = [word]\n",
    "    similar_vals = [1]\n",
    "    try:\n",
    "        similar_list = model.most_similar(positive=[word],topn=n)\n",
    "        for tupl in similar_list:\n",
    "            words.append(tupl[0])\n",
    "            similar_vals.append(tupl[1])\n",
    "    except:\n",
    "        pass    \n",
    "    return words, similar_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = pd.read_csv('prc_data.csv', sep='\\t')\n",
    "cvs = cvs.set_index('Unnamed: 0')\n",
    "prc_description = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_value = {}\n",
    "similar_words_needed = 2\n",
    "for word in prc_description.split():\n",
    "    similar_words, similarity = get_closest(word, similar_words_needed)\n",
    "    for i in range(len(similar_words)):\n",
    "        word_value[similar_words[i]] = word_value.get(similar_words[i], 0)+similarity[i]\n",
    "        print(similar_words[i], word_value[similar_words[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_cv = 150\n",
    "\n",
    "count = {}\n",
    "idf = {}\n",
    "for word in word_value.keys():\n",
    "    count[word] = 0\n",
    "    for i in range(no_of_cv):\n",
    "        try:\n",
    "            if word in cvs.loc(0)['skill'][i].split() or word in cvs.loc(0)['exp'][i].split():\n",
    "                count[word] += 1\n",
    "        except:\n",
    "            pass\n",
    "    if (count[word] == 0):\n",
    "        count[word] = 1\n",
    "    idf[word] = math.log(no_of_cv/count[word])\n",
    "print(count)\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = {}\n",
    "for i in range(no_of_cv):\n",
    "    score[i] = 0\n",
    "    try:\n",
    "        for word in word_value.keys():\n",
    "            tf = cvs.loc(0)['skill'][i].split().count(word) + cvs.loc(0)['exp'][i].split().count(word)\n",
    "            score[i] += word_value[word]*tf*idf[word]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = []\n",
    "for i in range(no_of_cv):\n",
    "    sorted_list.append((score[i], i))\n",
    "sorted_list.sort(reverse = True)\n",
    "for s, i in sorted_list:\n",
    "    if list(cvs)[i] != '.DS_Store':\n",
    "        print(list(cvs)[i], ':', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
